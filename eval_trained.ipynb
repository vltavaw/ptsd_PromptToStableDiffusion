{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyNftK5HB7PvQSsvt+Tcxh79"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"FCzDnM5Irq0R","executionInfo":{"status":"error","timestamp":1714408517483,"user_tz":240,"elapsed":137986,"user":{"displayName":"Blake Wang","userId":"06564175542065527029"}},"outputId":"4157e861-6076-4a0c-ac3c-ba1042301d5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n","Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.2.1+cu121)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1->torchvision)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1->torchvision)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1->torchvision)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1->torchvision)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1->torchvision)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1->torchvision)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1->torchvision)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1->torchvision)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1->torchvision)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1->torchvision)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1->torchvision)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchvision)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchvision) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchvision) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n","Collecting datasets\n","  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Collecting huggingface-hub>=0.21.2 (from datasets)\n","  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.20.3\n","    Uninstalling huggingface-hub-0.20.3:\n","      Successfully uninstalled huggingface-hub-0.20.3\n","Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.22.2 multiprocess-0.70.16 xxhash-3.4.1\n","Mounted at /content/drive\n","Collecting ftfy\n","  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.2.0\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-o047kp3r\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-o047kp3r\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.2.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.2.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.17.1+cu121)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369499 sha256=5057005f178b0b2727b25c33f7fcba96ee3581839ce2354b6ca269f850b35163\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-wvq4eymg/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 338M/338M [00:17<00:00, 20.0MiB/s]\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'device' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c640b983623b>\u001b[0m in \u001b[0;36m<cell line: 116>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ViT-B/32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m \u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0minput_resolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_resolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"]}],"source":["!pip install torchvision\n","!pip install transformers>=4.25.1\n","!pip install datasets\n","\n","from datasets import load_dataset, Image\n","import torch\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import json\n","from PIL import Image\n","\n","\n","#sys.path.append('/opt/cocoapi/PythonAPI')\n","\n","# https://github.com/L1aoXingyu/image-caption-project/blob/master/0_Dataset.ipynb\n","# returns a COCO object(which is dataset to be sampled from), coco_caps(captions), and the ids for COCO data instances\n","def load_coco():\n","    # initialize COCO API for instance annotations\n","    dataDir = '/content/gdrive/My Drive/cocoapi'\n","    dataType = 'val2014'\n","    instances_annFile = os.path.join(dataDir, 'annotations/instances_{}.json'.format(dataType))\n","    coco = COCO(instances_annFile)\n","\n","    # initialize COCO API for caption annotations\n","    captions_annFile = os.path.join(dataDir, 'annotations/captions_{}.json'.format(dataType))\n","    coco_caps = COCO(captions_annFile)\n","\n","    # get image ids\n","    ids = list(coco.anns.keys())\n","    return coco, coco_caps, ids\n","\n","\n","def sample_image(coco, coco_caps, ids, count):\n","    images = []\n","    captions = []\n","    while count > 0:\n","      count -= 1\n","      # pick a random image and obtain the corresponding URL\n","      ann_id = np.random.choice(ids)\n","      print(f\"ann_id: {ann_id}\")\n","      img_id = coco.anns[ann_id]['image_id']\n","      img = coco.loadImgs(img_id)[0]\n","      url = img['coco_url']\n","\n","    # print URL and visualize corresponding image\n","    #print(url)\n","    #I = io.imread(url)\n","    #plt.axis('off')\n","    #plt.imshow(I)\n","    #plt.show()\n","\n","    # load and display captions\n","      annIds = coco_caps.getAnnIds(imgIds=img['id']);\n","      anns = coco_caps.loadAnns(annIds)\n","      #coco_caps.showAnns(anns)\n","      images.append(img)\n","      captions.append(anns)\n","    return images, captions\n","\n","# generate instructions for LLM, feed into LLM, get optimized prompts\n","# few shot prompting: type/color of object, posture of object, place of object, placement relationship between objects, \"invent\" related unmentioned things!\n","def improve_captions(captions):\n","    improved_captions  = []\n","    for cap in captions:\n","        improved = \"This is \" + cap # TODO: d\n","        improved_captions.append(improved)\n","    return improved_captions\n","\n","def load_from_local():\n","    image_folder = '/content/drive/My Drive/C5470prj/coco_subset'\n","    file_path = image_folder + '/captions_with_improved.json'\n","    with open(file_path, 'r') as file:\n","        data = json.load(file)\n","    ids = []\n","    images = []\n","    captions = []\n","    improved_captions = []\n","    data = sorted(data, key=lambda x: x['image_id'])\n","    for item in data:\n","        ids.append(item['image_id'])\n","        captions.append(item['caption'])\n","        improved_captions.append(item['improved_caption'])\n","        image_id = item['image_id']\n","        filename = f'{image_id:012d}.jpg' # 12 digit padding\n","        full_path = os.path.join(image_folder, filename)\n","        try:\n","            img = Image.open(full_path)\n","            images.append(img)\n","        except FileNotFoundError:\n","            print(f\"Image file not found: {full_path}\")\n","    return ids, images, captions, improved_captions\n","\n","#coco, coco_caps, ids = load_coco()\n","#images, captions = sample_images(coco, coco_caps, ids, 1)\n","#improved_catpions = improve_captions(captions)\n","\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git\n","import clip\n","import skimage\n","import IPython.display\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from collections import OrderedDict\n","\n","import numpy as np\n","\n"]},{"cell_type":"code","source":["device = \"cuda\"\n","clip_model, preprocess = clip.load(\"ViT-B/32\")\n","clip_model.to(device)\n","clip_model.eval()\n","input_resolution = clip_model.visual.input_resolution\n","context_length = clip_model.context_length\n","vocab_size = clip_model.vocab_size\n","\n","print(f\"Model parameters: {np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()]):,}\")\n","print(\"Input resolution:\", input_resolution)\n","print(\"Context length:\", context_length)\n","print(\"Vocab size:\", vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uvKJoLvltgZr","executionInfo":{"status":"ok","timestamp":1714408542993,"user_tz":240,"elapsed":4083,"user":{"displayName":"Blake Wang","userId":"06564175542065527029"}},"outputId":"3b8aba67-f70a-448e-d64e-0af5f5f02ad1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Model parameters: 151,277,313\n","Input resolution: 224\n","Context length: 77\n","Vocab size: 49408\n"]}]},{"cell_type":"code","source":["from IPython.display import display\n","import fnmatch\n","\n","ids, images, captions, improved_captions = load_from_local()\n","\n","improved_images = []\n","improved_folder = '/content/drive/My Drive/C5470prj/improved_images'\n","\n","\n","def calculate_clip_scores(model, preprocess, images, captions):\n","    image_features = []\n","    for image in images:\n","        image_processed = preprocess(image).unsqueeze(0).to(device)\n","        image_features.append(model.encode_image(image_processed))\n","        #display(image)\n","\n","    print(captions)\n","\n","    text_tokens = clip.tokenize(captions).to(device)\n","    text_features = model.encode_text(text_tokens)\n","\n","    image_features = torch.cat(image_features)\n","    similarity = torch.matmul(image_features, text_features.t()).diag().cpu().detach().numpy()\n","    return similarity\n","\n","def print_stats(improved_images, captions, improved_scores):\n","    # show prompt-image pairs with highest/lowest score\n","    paired_scores = list(zip(improved_images, captions, improved_scores))\n","    paired_scores.sort(key=lambda x: x[2], reverse=True)\n","\n","    img, cap, sc = paired_scores[0]\n","    print(f\"Highest CLIP Score: {sc}, caption: {cap}, img: \")\n","    display(img)\n","    img, cap, sc = paired_scores[-1]\n","    print(f\"Lowest CLIP Score: {sc}, caption: {cap}, img: \")\n","    display(img)\n","\n","def load_and_sort(folder_path):\n","    images = []\n","    files = sorted(os.listdir(folder_path))\n","    for id in ids:\n","        patterns = [f'*{id}*.jpg', f'*{id}*.png']\n","        for file in files:\n","            if any(fnmatch.fnmatch(file, pattern) for pattern in patterns):\n","                images.append(Image.open(os.path.join(folder_path, file)))\n","                break\n","    return images\n","\n","folder_paths = {\n","    'improved_caption': '/content/drive/My Drive/C5470prj/improved_caption',\n","    'improved_images': '/content/drive/My Drive/C5470prj/improved_images',\n","    'original_caption': '/content/drive/My Drive/C5470prj/original_caption'\n","}\n","\n","improved_caption_images = load_and_sort(folder_paths['improved_caption'])\n","improved_images = load_and_sort(folder_paths['improved_images'])\n","original_caption_images = load_and_sort(folder_paths['original_caption'])\n","\n","\n","# calculate CLIP-SCORE, again\n","improved_scores = calculate_clip_scores(clip_model, preprocess, improved_caption_images, captions)\n","original_scores = calculate_clip_scores(clip_model, preprocess, original_caption_images, captions)\n","\n","print(f\"original avg score: {np.average(original_scores)}\")\n","\n","print(f\"imrpoved avg score: {np.average(improved_scores)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kts6yCdVr7gQ","executionInfo":{"status":"ok","timestamp":1714408804150,"user_tz":240,"elapsed":1480,"user":{"displayName":"Blake Wang","userId":"06564175542065527029"}},"outputId":"c1bf8451-3bfa-4380-9324-badb9661dee8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["['Two apple computers are on a white desk', 'a red and black train is coming down the tracks, with workers', 'Goat standing on top of rock pile with grass growing on it', 'A pizza served on a white plate next to empty wine glass and beer.', 'A laptop on a table outside a balcony overlooking the ocean.', 'A bathroom with a sink, toilet and tub in front of a large mirror.', 'A large cat sitting in a bathroom sink under a mirror, staring.', 'A cutting board and knife, with chopped carrots, celery, and onion.', 'some elephants and one is by some water', 'a metal bench resting on a sidewalk with cars on a street in the background', 'A stack of pancakes that are sitting on a plate.', 'A boat passes by waterfront houses flanked by trees.', 'A train passing through wooded areas on a train track.', 'The inside view of a large decorated church.', 'A bowl of Asian Cuisine with beef, noodles and broccoli.']\n","['Two apple computers are on a white desk', 'a red and black train is coming down the tracks, with workers', 'Goat standing on top of rock pile with grass growing on it', 'A pizza served on a white plate next to empty wine glass and beer.', 'A laptop on a table outside a balcony overlooking the ocean.', 'A bathroom with a sink, toilet and tub in front of a large mirror.', 'A large cat sitting in a bathroom sink under a mirror, staring.', 'A cutting board and knife, with chopped carrots, celery, and onion.', 'some elephants and one is by some water', 'a metal bench resting on a sidewalk with cars on a street in the background', 'A stack of pancakes that are sitting on a plate.', 'A boat passes by waterfront houses flanked by trees.', 'A train passing through wooded areas on a train track.', 'The inside view of a large decorated church.', 'A bowl of Asian Cuisine with beef, noodles and broccoli.']\n","original avg score: 18.046875\n","imrpoved avg score: 24.703125\n"]}]}]}